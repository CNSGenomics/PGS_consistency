---
title: "Chip Data from HSU"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    toc: TRUE
    toc_float: TRUE
    number_sections: TRUE

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
# Libraries
library(ggplot2)
library(dplyr)
library(ggrepel)
library(DT)
library(qqman)
library(tibble)

```

***


We have developed an in-house pipeline for processing raw plink data all the way to profiling their GPS of interested traits. 

The samples are genotyped by HSU, and exported into a raw data in Plink format, includes a MAP file and PED file, using Genome Studio. 

# Pre-imputation processing

## convert to binary format

We usually transfer the data to binary format to save space and accelerate calculation speed.  

```{bash, eval = F}

plink \
--file ${inpath}/${data}  \
--make-bed --out ${data}
```

This format is explained well in [plink manual page](https://zzz.bwh.harvard.edu/plink/data.shtml#bed).

## Quality

We can use Plink to generate summary stats at per-SNP and per-individual levels. 

```{bash, eval = F}
plink  --bfile ${data}  --missing   --out   ${data}
plink  --bfile ${data}  --freq      --out   ${data}
plink  --bfile ${data}  --hardy     --out   ${data}
```

We used R to read in and make histograms of the summary statistics. 

```{r, eval = F}
filename="target"

imiss <- read.table(paste0(filename,".imiss"), header=T, check.names=F)
lmiss <- read.table(paste0(filename,".lmiss"), header=T, check.names=F)
freq <- read.table(paste0(filename,".frq"), header=T,    check.names=F)
hwe <- read.table(paste0(filename,".hwe"), header=T,   check.names=F)

# You can either run the plotting function in interactive Jupyter R. 
# Or un-mute the png and dev.off command lines to save the picture. 

#png(paste0(filename,"_Quality_of_genotype.png"), type="cairo")
par(mfrow=c(2,2))
hist(1-imiss$F_MISS, breaks="sturges",main="Individuals",col="tan", 
     xlab="Genotyping Rate", ylab="Number of Individuals")
hist(1-lmiss$F_MISS, breaks="sturges", main="SNPs", col="tan", 
     xlab="Genotyping Rate", ylab="Number of SNPs")
hist(hwe$P, breaks="sturges", main="HWE P-Value", col="tan", 
     xlab="HWE P-value", ylab="Number of SNPs")
hist(freq$MAF, breaks="sturges", main="MAF", col="tan", 
     xlab="MAF", ylab="Number of SNPs")
#dev.off()
```


## Sex check

As a very useful way to confirm that the samples were not messed up in handling, we predict sex from heterozygosity in chromosome X, and compare it to the sex in phenotype column in the fam file. 

A PROBLEM arises if the two sexes do not match, or if the SNP data or pedigree data are ambiguous with regard to sex. More details are in [plink manual page](https://zzz.bwh.harvard.edu/plink/summary.shtml#sexcheck). 

We need chrX to do this check, but our predictor data has only autosomes, so we will leave out the X chromosome in next step. 

```{bash, eval = F}
# don't run this chunk with practical data. it does not have X chromosome
plink  --bfile ${data}  --check-sex --out  ${data}
```


## QC

In QC step, we will use Plink to exclude the SNPs and individuals with high missingess, very low MAF, and SNPs that violate Hardy-Weinberg Equilibrium. 

It's very often that our target data set has a small sample size, like one or two hundred. Some cross-sample QC methods don't work very well in this circumstance. 
Comparing to what we have learnt in the QC steps for a GWAS data set, which usually have thousands of samples, we are doing less filtering here. 


```{bash, eval = F}

plink  \
 --bfile  ${data}  \
 --mind 0.05 \
 --geno 0.05 \
 --hwe 0.000001 \
 --maf 0.01 \
 --make-bed  \
 --out   ${data}_qced
```


Parameters in the command:   
  --mind: missingness per individual threshold    
  --geno: missingness per SNP threshold  
  --hwe: Hardy-Weinberg Equilibrium p-value threshold  
  --maf: Minor allele frequency thresohld  


## Flip SNPs on minus Strand 

The chips don't always have probes on the positive strands. We have to flip all the SNPs on minus strand to positive strand for matching to reference data.
[Strand](https://www.strand.org.uk) website has prepared strand files for a lot of chips. We downloaded the strand data for each chip we used, and used the **update_build.sh** tool to flip the strands. 

```{bash, eval = F}


```

## SNP ID consensus

If you find a lot of SNPs in your data using customized IDs, in a form such as kpg1234, gsa.101 or chr1:10011, you can use the following R package to update the bim file.

```{r, eval = F}
## don't need to run this chunk. It's just for showing you the R package.
BiocManager::install("SNPlocs.Hsapiens.dbSNP144.GRCh37")
library(SNPlocs.Hsapiens.dbSNP144.GRCh37) 
ref <- SNPlocs.Hsapiens.dbSNP144.GRCh37
```


## Align to reference genome

For imputation purpose, we will align the SNPs to reference human genome, which is to make sure that the SNPs in our data and the reference data have the same ID, same location, and the alleles are read from the same strand.



```{r, out.width = "300px", echo = F, fig.align='center', eval = F}
knitr::include_graphics("Figures/strand_display.png")
```

Our example data has already got all SNPs aligned. We will skip this section in practice, but it's very important to check through each aspect and process your data carefully in your research.   





# Data Imputation


There are several online imputation servers you can use to impute your data, such as [TOPMED imputation server](https://imputation.biodatacatalyst.nhlbi.nih.gov/#!) and [Sanger imputation server](https://imputation.sanger.ac.uk). Here we will use open resource tools and the reference data [1000Genome](https://www.internationalgenome.org)  to do it in-house. 

## Fix reference allele

We will convert the data from PLINK format to VCF format, and use BCFTools to align the reference alleles as used in human genome reference data.

```{bash, eval = F}
chr=22

# Pull out data for relevant chromosome and convert to VCF. 
plink --bfile ${data}_chr${chr}  --recode vcf --out  ${data}_chr${chr}

# Sort and compress the VCF file
vcf-sort ${data}_chr${chr}.vcf | bgzip -c > ${data}_chr${chr}.vcf.gz

# Fix the reference allele to match the GRCh37 reference fasta (human_g1k_v37.fasta). 
ref2fix=${refpath}/human_g1k_v37.fasta
BCFTOOLS_PLUGINS=/software/bin/
bcftools \
  +fixref \
  ${data}_chr${chr}.vcf.gz \
  -Oz \
  -o fixed_${data}_chr${chr}.vcf.gz  \
  -- -d \
  -f ${ref2fix} \
  -m flip

zcat fixed_${data}_chr${chr}.vcf.gz | bgzip -c > indexed_fixed_${data}_chr${chr}.vcf.gz

# create index file. 
tabix indexed_fixed_${data}_chr${chr}.vcf.gz
```


## Phasing

Although it is not required for all imputation softwares, here we will reconstruct the haplotypes from our data with external information, which is called phasing.

Both of the haplotype reference and genetic map used here are from [1000Genome](https://www.internationalgenome.org) project. 

There are many phasing tools. We will use [Eagle v2.4.1](https://alkesgroup.broadinstitute.org/Eagle/) in our practice. 


```{bash, eval = F}
geneticmap1=${refpath}/genetic_map_chr${chr}_combined_b37_modified.txt
reference=${refpath}/ALL.chr${chr}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz

# Use EAGLE to generate phased haplotypes
# it takes about 4 minutes.
eagle  \
    --vcfRef=$reference  \
    --vcfTarget=indexed_fixed_${data}_chr${chr}.vcf.gz  \
    --geneticMapFile=$geneticmap1  \
    --vcfOutFormat=z \
    --outPrefix=phased_chr${chr} > phasing.log

# index the vcf.gz file
tabix -p vcf  phased_chr${chr}.vcf.gz
```


You can take a look at the phased vcf file with zless. 

```{bash, eval = F}
zless phased_chr${chr}.vcf.gz | less -S

# less -S will align the columns for you, so the file is clearer to visualize. 
# press q to exit from the file
```

## Imputation

We use Impute5 to impute the phased data. To reduce computation time, we will only imputed a chunk of chromosome 22. 

```{bash, eval = F}
# CAUTION: genetic map file is different from the one used in phasing!
# impute5 doesn't want the chr column in genetic map, so we removed that column

# imputing chr22 takes around 8min. If you are in a hurry, try with next chunk. 

geneticmap2=${refpath}/genetic_map_chr${chr}_combined_b37.txt
reference=${refpath}/ALL.chr${chr}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz

impute5_1.1.5_static \
        --m  $geneticmap2 \
        --h  $reference  \
        --g  phased_chr${chr}.vcf.gz  \
        --r  ${chr} \
        --ne 20000   \
        --threads 1 \
        --o  imputed_chr${chr}.vcf.gz \
        --l  imputed_chr${chr}.log
```



If the sample size is big, we can run imputation by chunks and parallel them. 

```{bash , eval = F}
# set the boundary of the chunk to impute
intstart=40000001 
intend=50000000

impute5_1.1.5_static \
        --m  $geneticmap2 \
        --h  $reference  \
        --g  phased_chr${chr}.vcf.gz  \
        --r  ${chr}:${intstart}-${intend} \
        --ne 20000   \
        --threads 1 \
        --o  imputed_chr${chr}_chunk.vcf.gz \
        --l  imputed_chr${chr}_chunk.log

```

## QC and format the imputed data

Imputed data is output as a zipped VCF file. We will change the format back to PLINK for following analysis.   

We will use BCFTOOL again to extract info score for the imputed SNPs from the VCF file, which stands for the imputation quality per SNP. Info score is sensitive to sample size, so be careful to use it when you have a very small sample size in real studies.  

1000G included a lot of InDels, which sometimes use the same SNP ID. Duplicated SNP IDs will introduce error, so we will simply remove them in our practice.

If you are interested in these InDels or any SNPs with duplicate ID or missing ID, you can refill the bim file in R in the form of chr1:123456 or chr1:1234_dup2, since PLINK can't deal with duplicated or missing SNP IDs. 

```{bash, eval = F}
## convert the format using plink
plink --vcf  imputed_chr${chr}.vcf.gz   \
            --id-delim '_'  \
            --keep-allele-order \
            --make-bed \
            --out  imputed_chr${chr}

## get info score, need bcftools
tabix -p vcf   imputed_chr${chr}.vcf.gz
bcftools query -f '%CHROM\t%ID\t%QUAL\t%POS\t%REF\t%ALT\t%INFO/AF\t%INFO/INFO\n' \
imputed_chr${chr}.vcf.gz > imputed_chr${chr}.info

## get the list of SNPs with info > 0.3
awk ' $8 > 0.3 ' imputed_chr${chr}.info | awk '{print $2}' > snps_with_invo_over_0.3.info

## QC with info score 
plink2 --bfile imputed_chr${chr}  \
--extract snps_with_invo_over_0.3.info  \
--rm-dup force-first  \
--make-bed \
--out imputed_chr${chr}_QCed

## note that we are using plink2 here, since plink does not have function --rm-dup. 
## plink and plink2 have slight difference, and each of them have a few specific functions. We are more used to plink, so I kept to plink for all the other steps. 

```



When we work with the **whole genome**, we can use the following example commands to merge imputed chunks and chromosome. 

```{bash, eval = F}
## don't run this chunk. It's just an example, without usable inputs. 

# in vcf format
filelist=$(ls  imputed_chr*vcf.gz | tr '\n'   '\t')  
bcftools concat -Oz -o ${data}_imputed.vcf.gz  ${filelist}

# in plink format
plink \
--bfile imputed_chr1_QCed  \
--merge-list file_names_of_imputed_chr_2_to_22.txt \
--allow-no-sex  \
--make-bed  \
--out  ${data}_imputed_qced_autosomes 
```

Sometimes we would like to keep all tha SNPs no matter of the imputation quality, so that we won't have missing SNPs which are in the predictor. 



# PGS profiling

```{bash, eval = F}
ls /QRISdata/Q3046/Data/*/grp_pipeline_*_impute5_bunya/Imputed_plink_format/*_autosomes.bim
```


Taking the imputed data, we will profile the PGS scores using PLINK.


```{bash, eval = F}
plink  \
--bfile  $bfile \
--score  ${predictor}  2 5 8  header sum    \
--out  $output
```


The parameters after your predictor file means

 + 1 2 3: Take only the first three columns in the predictor file.   
 + header: The predictor file has a header row.  
 + sum: Plink prefers to divide the score by the number of SNPs in predictor. Using "sum" will prevent the division step.   



```{bash, eval = F}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=50G
#SBATCH --time=5:00:00
#SBATCH --job-name=profiling
#SBATCH --error=/scratch/project/genetic_data_analysis/uqtlin5/CEPH_samples/CHIP/slurm_%A_%a.error
#SBATCH --output=/scratch/project/genetic_data_analysis/uqtlin5/CEPH_samples/CHIP/slurm_%A_%a.out
#SBATCH --partition=general
#SBATCH --account=a_mcrae
#SBATCH --array=1-7


i=$SLURM_ARRAY_TASK_ID


cd /scratch/project/genetic_data_analysis/uqtlin5/CEPH_samples/CHIP/

samplefile=all_grp_v4_data.txt 


## define predictor
#traitfile="/QRISdata/Q6913/GCTB_predictor_list_for_batch_profiling.txt"
#traitfile="traits.not.profiled.yet.txt"

traitfile="traits.to.rerun.for.SARSCOV2.txt"
trait=$(sed "${i}q;d" $traitfile | awk '{print $1}' )
predictor=$(sed  "${i}q;d"  $traitfile  | awk '{print $3}' )
echo $trait
echo $predictor


outdir=PRS_all_GCTB
mkdir -p $outdir

## define data

for j in {1..74}; do 
# j=75
input=$(sed "${j}q;d"  $samplefile  | awk '{print $4}'  )	
		cohort=$(sed "${j}q;d"  $samplefile  | awk '{print $1}' )
		version=$(sed "${j}q;d"  $samplefile  | awk '{print $2}' )
		data=$(sed "${j}q;d"  $samplefile  | awk '{print $3}'  )

		plink \
	 	  --bfile  PLINK/${cohort}/${data}  \
	 	  --score  $predictor  2 5 8  header sum    \
	  	 --out ${outdir}/${cohort}_${version}_${trait}_SBayesRC 	
done

```

# Merge the files


```{r, eval = F}

cohorts.file = read.table("all_grp_v4_data.txt")

trait.list = read.table("/QRISdata/Q6913/GCTB_predictor_list_for_batch_profiling.txt")
n.trait = nrow(trait.list)
traitArray=trait.list$V1

group="PRS_all_GCTB"

#for (i in 1:nrow(cohorts.file)) {

i=75
cohort=cohorts.file[i,"V1"]
pipeline=cohorts.file[i,"V2"]
data = cohorts.file[i,"V3"]

fam=paste0("PLINK/", cohort , "/", data)

target.data =read.table(paste0(fam, ".fam")) 
target.data = target.data[,c(1,2)]
colnames(target.data) = c("FID", "IID")

target.data$batch = cohort
target.data$pipeline = pipeline
  
for (j in 1:length(traitArray)){
  trait = traitArray[j]
  file.name =  paste0( group, "/",cohort, "_",  pipeline, "_", trait, "_SBayesRC.profile")
  profile = read.table(file.name, header = T)
  target.data$new.column = profile[match(target.data$IID, profile$IID),"SCORESUM"]
  colnames(target.data)[ncol(target.data)] = trait
  }
  
row.names(target.data) = target.data$IID
target.data = target.data[,3:ncol(target.data)]
write.csv(target.data, file= paste0( group, "_",cohort, "_all_", n.trait , "_traits_GCTB_PRS.csv"))

# }

```




```{r, eval = F}
cohorts.file = read.table("all_grp_v4_data.txt")
group="PRS_all_GCTB"

merged.data=data.frame()
for (i in 1:nrow(cohorts.file)) {
cohort=cohorts.file[i,"V1"]
pipeline=cohorts.file[i,"V2"]
data = cohorts.file[i,"V3"]
target.data = read.csv(paste0( group, "_",cohort, "_all_", n.trait , "_traits_GCTB_PRS.csv"))
merged.data =  rbind(merged.data, target.data)
}
write.csv(merged.data, file = "GRP_all_GCTB_PRS.csv")

```

```{r, eval = F, echo = F}

## fix the 35BM traits.
merged.data = read.csv("Data/GRP/GRP_all_GCTB_PRS.csv", row.names = 1)
merged.data[,grep("UKB_35BM", colnames(merged.data))] =  -merged.data[,grep("UKB_35BM", colnames(merged.data))] 

write.csv(merged.data,  "Data/GRP/GRP_all_GCTB_PRS.csv", row.names = F)

```


































# sample size

```{r}

grp.raw = read.csv("Data/GRP/GRP_all_GCTB_PRS.csv")
batch.summary = data.frame(table(grp.raw$batch))
dim(grp.raw)

length(table(grp.raw$batch))

hist(batch.summary$Freq)
```









